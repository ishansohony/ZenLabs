{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Fraud</th>\n",
       "      <th>Normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9   ...         V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787   ...    0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425   ...   -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654   ...    0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024   ...    0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739   ...    0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Fraud  Normal  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0     1.0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0     1.0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0     1.0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0     1.0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0     1.0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/nodlehs/Downloads/creditcard.csv\")\n",
    "df.loc[df.Class == 0, 'Normal'] = 1\n",
    "df.loc[df.Class == 1, 'Normal'] = 0\n",
    "df = df.rename(columns={'Class': 'Fraud'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    284315\n",
      "0.0       492\n",
      "Name: Normal, dtype: int64\n",
      "\n",
      "0    284315\n",
      "1       492\n",
      "Name: Fraud, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.Normal.value_counts())\n",
    "print()\n",
    "print(df.Fraud.value_counts())\n",
    "Fraud = df[df.Fraud == 1]\n",
    "Normal = df[df.Normal == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = Fraud[::2]\n",
    "count_Frauds = len(X_train)\n",
    "\n",
    "X_train = pd.concat([X_train, Normal.sample(n = 50000)], axis = 0)\n",
    "\n",
    "X_test = df.loc[~df.index.isin(X_train.index)]\n",
    "\n",
    "X_train = shuffle(X_train)\n",
    "X_test = shuffle(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train = X_train.Fraud\n",
    "y_train = pd.concat([y_train, X_train.Normal], axis=1)\n",
    "\n",
    "y_test = X_test.Fraud\n",
    "y_test = pd.concat([y_test, X_test.Normal], axis=1)\n",
    "\n",
    "X_train = X_train.drop(['Fraud','Normal'], axis = 1)\n",
    "X_test = X_test.drop(['Fraud','Normal'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50246\n",
      "50246\n",
      "234561\n",
      "234561\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(len(X_test))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "ratio will act as an equal weighting system for our model. Because we want to reduce the mean squared error between\n",
    "our predicted and target values, we want the initial sums of our labels to be equal. ratio will multiply the Fraud \n",
    "values so that they equal the sum of the Normal values (each Normal value equals 1).\n",
    "'''\n",
    "ratio = len(X_train)/count_Frauds \n",
    "\n",
    "y_train.Fraud *= ratio\n",
    "y_test.Fraud *= ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = X_train.columns.values\n",
    "\n",
    "for feature in features:\n",
    "    mean, std = df[feature].mean(), df[feature].std()\n",
    "    X_train.loc[:, feature] = (X_train[feature] - mean) / std\n",
    "    X_test.loc[:, feature] = (X_test[feature] - mean) / std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputX = X_train.as_matrix()\n",
    "inputY = y_train.as_matrix()\n",
    "inputX_test = X_test.as_matrix()\n",
    "inputY_test = y_test.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mulitplier = 1.5 \n",
    "\n",
    "\n",
    "hidden_nodes1 = 15\n",
    "hidden_nodes2 = round(hidden_nodes1 * mulitplier)\n",
    "hidden_nodes3 = round(hidden_nodes2 * mulitplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 30]) #there are 30 inputs\n",
    "\n",
    "#layer 1\n",
    "W1 = tf.Variable(tf.zeros([30, hidden_nodes1]))\n",
    "b1 = tf.Variable(tf.zeros([hidden_nodes1]))\n",
    "y1 = tf.nn.sigmoid(tf.matmul(x, W1) + b1)\n",
    "\n",
    "#layer 2\n",
    "W2 = tf.Variable(tf.zeros([hidden_nodes1, hidden_nodes2]))\n",
    "b2 = tf.Variable(tf.zeros([hidden_nodes2]))\n",
    "y2 = tf.nn.sigmoid(tf.matmul(y1, W2) + b2)\n",
    "\n",
    "#layer 3\n",
    "W3 = tf.Variable(tf.zeros([hidden_nodes2, hidden_nodes3])) \n",
    "b3 = tf.Variable(tf.zeros([hidden_nodes3]))\n",
    "y3 = tf.nn.sigmoid(tf.matmul(y2, W3) + b3)\n",
    "\n",
    "#layer 4\n",
    "W4 = tf.Variable(tf.zeros([hidden_nodes3, 2])) \n",
    "b4 = tf.Variable(tf.zeros([2]))\n",
    "y4 = tf.nn.softmax(tf.matmul(y3, W4) + b4)\n",
    "\n",
    "#output\n",
    "y = y4\n",
    "y_ = tf.placeholder(tf.float32, [None, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0005\n",
    "training_epochs = 30000\n",
    "display_step = 100\n",
    "n_samples = y_train.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.square(y_ - y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step: 0 Accuracy = 0.00490 Cost =  101.89774\n",
      "Training step: 100 Accuracy = 0.00490 Cost =  101.89889\n",
      "Training step: 200 Accuracy = 0.00490 Cost =  101.80441\n",
      "Training step: 300 Accuracy = 0.02076 Cost =  101.62411\n",
      "Training step: 400 Accuracy = 0.84837 Cost =  101.49538\n",
      "Training step: 500 Accuracy = 0.93170 Cost =  101.42694\n",
      "Training step: 600 Accuracy = 0.94883 Cost =  101.38492\n",
      "Training step: 700 Accuracy = 0.95767 Cost =  101.32066\n",
      "Training step: 800 Accuracy = 0.96199 Cost =  101.28936\n",
      "Training step: 900 Accuracy = 0.96531 Cost =  101.26963\n",
      "Training step: 1000 Accuracy = 0.96913 Cost =  101.25553\n",
      "Training step: 1100 Accuracy = 0.97178 Cost =  101.23685\n",
      "Training step: 1200 Accuracy = 0.97401 Cost =  101.22544\n",
      "Training step: 1300 Accuracy = 0.97502 Cost =  101.21721\n",
      "Training step: 1400 Accuracy = 0.97654 Cost =  101.21067\n",
      "Training step: 1500 Accuracy = 0.97787 Cost =  101.20540\n",
      "Training step: 1600 Accuracy = 0.97912 Cost =  101.20078\n",
      "Training step: 1700 Accuracy = 0.97974 Cost =  101.19552\n",
      "Training step: 1800 Accuracy = 0.98099 Cost =  101.19186\n",
      "Training step: 1900 Accuracy = 0.98165 Cost =  101.18884\n",
      "Training step: 2000 Accuracy = 0.98177 Cost =  101.18624\n",
      "Training step: 2100 Accuracy = 0.98181 Cost =  101.18381\n",
      "Training step: 2200 Accuracy = 0.98217 Cost =  101.18172\n",
      "Training step: 2300 Accuracy = 0.98296 Cost =  101.17970\n",
      "Training step: 2400 Accuracy = 0.98404 Cost =  101.17709\n",
      "Training step: 2500 Accuracy = 0.98485 Cost =  101.17522\n",
      "Training step: 2600 Accuracy = 0.98565 Cost =  101.17367\n",
      "Training step: 2700 Accuracy = 0.98653 Cost =  101.17215\n",
      "Training step: 2800 Accuracy = 0.98726 Cost =  101.17071\n",
      "Training step: 2900 Accuracy = 0.98796 Cost =  101.16936\n",
      "Training step: 3000 Accuracy = 0.98878 Cost =  101.16814\n",
      "Training step: 3100 Accuracy = 0.98913 Cost =  101.16707\n",
      "Training step: 3200 Accuracy = 0.98983 Cost =  101.16611\n",
      "Training step: 3300 Accuracy = 0.99079 Cost =  101.16510\n",
      "Training step: 3400 Accuracy = 0.99148 Cost =  101.16433\n",
      "Training step: 3500 Accuracy = 0.99204 Cost =  101.16361\n",
      "Training step: 3600 Accuracy = 0.99250 Cost =  101.16298\n",
      "Training step: 3700 Accuracy = 0.99295 Cost =  101.16230\n",
      "Training step: 3800 Accuracy = 0.99333 Cost =  101.16175\n",
      "Training step: 3900 Accuracy = 0.99371 Cost =  101.16119\n",
      "Training step: 4000 Accuracy = 0.99393 Cost =  101.16066\n",
      "Training step: 4100 Accuracy = 0.99419 Cost =  101.16020\n",
      "Training step: 4200 Accuracy = 0.99441 Cost =  101.15974\n",
      "Training step: 4300 Accuracy = 0.99471 Cost =  101.15935\n",
      "Training step: 4400 Accuracy = 0.99506 Cost =  101.15892\n",
      "Training step: 4500 Accuracy = 0.99528 Cost =  101.15857\n",
      "Training step: 4600 Accuracy = 0.99552 Cost =  101.15826\n",
      "Training step: 4700 Accuracy = 0.99586 Cost =  101.15791\n",
      "Training step: 4800 Accuracy = 0.99598 Cost =  101.15762\n",
      "Training step: 4900 Accuracy = 0.99608 Cost =  101.15740\n",
      "Training step: 5000 Accuracy = 0.99620 Cost =  101.15723\n",
      "Training step: 5100 Accuracy = 0.99634 Cost =  101.15703\n",
      "Training step: 5200 Accuracy = 0.99652 Cost =  101.15683\n",
      "Training step: 5300 Accuracy = 0.99664 Cost =  101.15664\n",
      "Training step: 5400 Accuracy = 0.99672 Cost =  101.15646\n",
      "Training step: 5500 Accuracy = 0.99684 Cost =  101.15632\n",
      "Training step: 5600 Accuracy = 0.99692 Cost =  101.15615\n",
      "Training step: 5700 Accuracy = 0.99699 Cost =  101.15601\n",
      "Training step: 5800 Accuracy = 0.99709 Cost =  101.15588\n",
      "Training step: 5900 Accuracy = 0.99717 Cost =  101.15575\n",
      "Training step: 6000 Accuracy = 0.99723 Cost =  101.15566\n",
      "Training step: 6100 Accuracy = 0.99735 Cost =  101.15556\n",
      "Training step: 6200 Accuracy = 0.99739 Cost =  101.15549\n",
      "Training step: 6300 Accuracy = 0.99747 Cost =  101.15540\n",
      "Training step: 6400 Accuracy = 0.99755 Cost =  101.15534\n",
      "Training step: 6500 Accuracy = 0.99759 Cost =  101.15527\n",
      "Training step: 6600 Accuracy = 0.99761 Cost =  101.15522\n",
      "Training step: 6700 Accuracy = 0.99761 Cost =  101.15514\n",
      "Training step: 6800 Accuracy = 0.99763 Cost =  101.15508\n",
      "Training step: 6900 Accuracy = 0.99765 Cost =  101.15503\n",
      "Training step: 7000 Accuracy = 0.99765 Cost =  101.15499\n",
      "Training step: 7100 Accuracy = 0.99767 Cost =  101.15494\n",
      "Training step: 7200 Accuracy = 0.99773 Cost =  101.15486\n",
      "Training step: 7300 Accuracy = 0.99781 Cost =  101.15477\n",
      "Training step: 7400 Accuracy = 0.99785 Cost =  101.15473\n",
      "Training step: 7500 Accuracy = 0.99789 Cost =  101.15465\n",
      "Training step: 7600 Accuracy = 0.99789 Cost =  101.15459\n",
      "Training step: 7700 Accuracy = 0.99793 Cost =  101.15453\n",
      "Training step: 7800 Accuracy = 0.99795 Cost =  101.15445\n",
      "Training step: 7900 Accuracy = 0.99799 Cost =  101.15438\n",
      "Training step: 8000 Accuracy = 0.99803 Cost =  101.15429\n",
      "Training step: 8100 Accuracy = 0.99801 Cost =  101.15421\n",
      "Training step: 8200 Accuracy = 0.99799 Cost =  101.15410\n",
      "Training step: 8300 Accuracy = 0.99801 Cost =  101.15400\n",
      "Training step: 8400 Accuracy = 0.99799 Cost =  101.15393\n",
      "Training step: 8500 Accuracy = 0.99793 Cost =  101.15385\n",
      "Training step: 8600 Accuracy = 0.99711 Cost =  101.15377\n",
      "Training step: 8700 Accuracy = 0.99656 Cost =  101.15363\n",
      "Training step: 8800 Accuracy = 0.99628 Cost =  101.15350\n",
      "Training step: 8900 Accuracy = 0.99610 Cost =  101.15321\n",
      "Training step: 9000 Accuracy = 0.99598 Cost =  101.15295\n",
      "Training step: 9100 Accuracy = 0.99612 Cost =  101.15269\n",
      "Training step: 9200 Accuracy = 0.99640 Cost =  101.15242\n",
      "Training step: 9300 Accuracy = 0.99658 Cost =  101.15220\n",
      "Training step: 9400 Accuracy = 0.99672 Cost =  101.15208\n",
      "Training step: 9500 Accuracy = 0.99682 Cost =  101.15199\n",
      "Training step: 9600 Accuracy = 0.99690 Cost =  101.15189\n",
      "Training step: 9700 Accuracy = 0.99703 Cost =  101.15179\n",
      "Training step: 9800 Accuracy = 0.99711 Cost =  101.15168\n",
      "Training step: 9900 Accuracy = 0.99715 Cost =  101.15161\n",
      "Training step: 10000 Accuracy = 0.99719 Cost =  101.15154\n",
      "Training step: 10100 Accuracy = 0.99723 Cost =  101.15147\n",
      "Training step: 10200 Accuracy = 0.99739 Cost =  101.15135\n",
      "Training step: 10300 Accuracy = 0.99745 Cost =  101.15128\n",
      "Training step: 10400 Accuracy = 0.99755 Cost =  101.15121\n",
      "Training step: 10500 Accuracy = 0.99759 Cost =  101.15117\n",
      "Training step: 10600 Accuracy = 0.99767 Cost =  101.15112\n",
      "Training step: 10700 Accuracy = 0.99769 Cost =  101.15110\n",
      "Training step: 10800 Accuracy = 0.99771 Cost =  101.15105\n",
      "Training step: 10900 Accuracy = 0.99773 Cost =  101.15105\n",
      "Training step: 11000 Accuracy = 0.99775 Cost =  101.15101\n",
      "Training step: 11100 Accuracy = 0.99777 Cost =  101.15098\n",
      "Training step: 11200 Accuracy = 0.99783 Cost =  101.15092\n",
      "Training step: 11300 Accuracy = 0.99787 Cost =  101.15089\n",
      "Training step: 11400 Accuracy = 0.99789 Cost =  101.15089\n",
      "Training step: 11500 Accuracy = 0.99789 Cost =  101.15087\n",
      "Training step: 11600 Accuracy = 0.99789 Cost =  101.15085\n",
      "Training step: 11700 Accuracy = 0.99791 Cost =  101.15082\n",
      "Training step: 11800 Accuracy = 0.99791 Cost =  101.15082\n",
      "Training step: 11900 Accuracy = 0.99795 Cost =  101.15081\n",
      "Training step: 12000 Accuracy = 0.99793 Cost =  101.15079\n",
      "Training step: 12100 Accuracy = 0.99799 Cost =  101.15075\n",
      "Training step: 12200 Accuracy = 0.99799 Cost =  101.15074\n",
      "Training step: 12300 Accuracy = 0.99799 Cost =  101.15073\n",
      "Training step: 12400 Accuracy = 0.99799 Cost =  101.15071\n",
      "Training step: 12500 Accuracy = 0.99799 Cost =  101.15070\n",
      "Training step: 12600 Accuracy = 0.99799 Cost =  101.15067\n",
      "Training step: 12700 Accuracy = 0.99775 Cost =  101.15002\n",
      "Training step: 12800 Accuracy = 0.99612 Cost =  101.14904\n",
      "Training step: 12900 Accuracy = 0.99660 Cost =  101.14830\n",
      "Training step: 13000 Accuracy = 0.99695 Cost =  101.14793\n",
      "Training step: 13100 Accuracy = 0.99705 Cost =  101.14768\n",
      "Training step: 13200 Accuracy = 0.99729 Cost =  101.14749\n",
      "Training step: 13300 Accuracy = 0.99743 Cost =  101.14735\n",
      "Training step: 13400 Accuracy = 0.99755 Cost =  101.14726\n",
      "Training step: 13500 Accuracy = 0.99767 Cost =  101.14719\n",
      "Training step: 13600 Accuracy = 0.99769 Cost =  101.14713\n",
      "Training step: 13700 Accuracy = 0.99769 Cost =  101.14711\n",
      "Training step: 13800 Accuracy = 0.99771 Cost =  101.14708\n",
      "Training step: 13900 Accuracy = 0.99775 Cost =  101.14702\n",
      "Training step: 14000 Accuracy = 0.99781 Cost =  101.14700\n",
      "Training step: 14100 Accuracy = 0.99785 Cost =  101.14695\n",
      "Training step: 14200 Accuracy = 0.99785 Cost =  101.14693\n",
      "Training step: 14300 Accuracy = 0.99785 Cost =  101.14692\n",
      "Training step: 14400 Accuracy = 0.99785 Cost =  101.14690\n",
      "Training step: 14500 Accuracy = 0.99791 Cost =  101.14687\n",
      "Training step: 14600 Accuracy = 0.99793 Cost =  101.14686\n",
      "Training step: 14700 Accuracy = 0.99795 Cost =  101.14684\n",
      "Training step: 14800 Accuracy = 0.99797 Cost =  101.14682\n",
      "Training step: 14900 Accuracy = 0.99797 Cost =  101.14682\n",
      "Training step: 15000 Accuracy = 0.99797 Cost =  101.14680\n",
      "Training step: 15100 Accuracy = 0.99797 Cost =  101.14680\n",
      "Training step: 15200 Accuracy = 0.99797 Cost =  101.14679\n",
      "Training step: 15300 Accuracy = 0.99797 Cost =  101.14679\n",
      "Training step: 15400 Accuracy = 0.99797 Cost =  101.14677\n",
      "Training step: 15500 Accuracy = 0.99797 Cost =  101.14676\n",
      "Training step: 15600 Accuracy = 0.99797 Cost =  101.14676\n",
      "Training step: 15700 Accuracy = 0.99797 Cost =  101.14676\n",
      "Training step: 15800 Accuracy = 0.99797 Cost =  101.14676\n",
      "Training step: 15900 Accuracy = 0.99797 Cost =  101.14676\n",
      "Training step: 16000 Accuracy = 0.99799 Cost =  101.14676\n",
      "Training step: 16100 Accuracy = 0.99799 Cost =  101.14676\n",
      "Training step: 16200 Accuracy = 0.99799 Cost =  101.14676\n",
      "Training step: 16300 Accuracy = 0.99799 Cost =  101.14675\n",
      "Training step: 16400 Accuracy = 0.99799 Cost =  101.14674\n",
      "Training step: 16500 Accuracy = 0.99801 Cost =  101.14673\n",
      "Training step: 16600 Accuracy = 0.99801 Cost =  101.14672\n",
      "Training step: 16700 Accuracy = 0.99801 Cost =  101.14672\n",
      "Training step: 16800 Accuracy = 0.99803 Cost =  101.14669\n",
      "Training step: 16900 Accuracy = 0.99803 Cost =  101.14669\n",
      "Training step: 17000 Accuracy = 0.99803 Cost =  101.14669\n",
      "Training step: 17100 Accuracy = 0.99803 Cost =  101.14669\n",
      "Training step: 17200 Accuracy = 0.99803 Cost =  101.14669\n",
      "Training step: 17300 Accuracy = 0.99803 Cost =  101.14668\n",
      "Training step: 17400 Accuracy = 0.99803 Cost =  101.14669\n",
      "Training step: 17500 Accuracy = 0.99803 Cost =  101.14668\n",
      "Training step: 17600 Accuracy = 0.99803 Cost =  101.14668\n",
      "Training step: 17700 Accuracy = 0.99803 Cost =  101.14668\n",
      "Training step: 17800 Accuracy = 0.99803 Cost =  101.14668\n",
      "Training step: 17900 Accuracy = 0.99803 Cost =  101.14668\n",
      "Training step: 18000 Accuracy = 0.99805 Cost =  101.14666\n",
      "Training step: 18100 Accuracy = 0.99805 Cost =  101.14665\n",
      "Training step: 18200 Accuracy = 0.99805 Cost =  101.14665\n",
      "Training step: 18300 Accuracy = 0.99805 Cost =  101.14665\n",
      "Training step: 18400 Accuracy = 0.99805 Cost =  101.14665\n",
      "Training step: 18500 Accuracy = 0.99805 Cost =  101.14665\n",
      "Training step: 18600 Accuracy = 0.99805 Cost =  101.14665\n",
      "Training step: 18700 Accuracy = 0.99805 Cost =  101.14665\n",
      "Training step: 18800 Accuracy = 0.99805 Cost =  101.14665\n",
      "Training step: 18900 Accuracy = 0.99805 Cost =  101.14665\n",
      "Training step: 19000 Accuracy = 0.99805 Cost =  101.14665\n",
      "Training step: 19100 Accuracy = 0.99805 Cost =  101.14664\n",
      "Training step: 19200 Accuracy = 0.99809 Cost =  101.14661\n",
      "Training step: 19300 Accuracy = 0.99809 Cost =  101.14661\n",
      "Training step: 19400 Accuracy = 0.99809 Cost =  101.14661\n",
      "Training step: 19500 Accuracy = 0.99809 Cost =  101.14661\n",
      "Training step: 19600 Accuracy = 0.99809 Cost =  101.14661\n",
      "Training step: 19700 Accuracy = 0.99811 Cost =  101.14659\n",
      "Training step: 19800 Accuracy = 0.99811 Cost =  101.14658\n",
      "Training step: 19900 Accuracy = 0.99811 Cost =  101.14658\n",
      "Training step: 20000 Accuracy = 0.99811 Cost =  101.14658\n",
      "Training step: 20100 Accuracy = 0.99811 Cost =  101.14658\n",
      "Training step: 20200 Accuracy = 0.99811 Cost =  101.14658\n",
      "Training step: 20300 Accuracy = 0.99811 Cost =  101.14658\n",
      "Training step: 20400 Accuracy = 0.99813 Cost =  101.14656\n",
      "Training step: 20500 Accuracy = 0.99813 Cost =  101.14656\n",
      "Training step: 20600 Accuracy = 0.99813 Cost =  101.14656\n",
      "Training step: 20700 Accuracy = 0.99813 Cost =  101.14656\n",
      "Training step: 20800 Accuracy = 0.99813 Cost =  101.14656\n",
      "Training step: 20900 Accuracy = 0.99813 Cost =  101.14656\n",
      "Training step: 21000 Accuracy = 0.99813 Cost =  101.14656\n",
      "Training step: 21100 Accuracy = 0.99813 Cost =  101.14656\n",
      "Training step: 21200 Accuracy = 0.99813 Cost =  101.14656\n",
      "Training step: 21300 Accuracy = 0.99813 Cost =  101.14656\n",
      "Training step: 21400 Accuracy = 0.99813 Cost =  101.14656\n",
      "Training step: 21500 Accuracy = 0.99813 Cost =  101.14656\n",
      "Training step: 21600 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 21700 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 21800 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 21900 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 22000 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 22100 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 22200 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 22300 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 22400 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 22500 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 22600 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 22700 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 22800 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 22900 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 23000 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 23100 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 23200 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 23300 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 23400 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 23500 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 23600 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 23700 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 23800 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 23900 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 24000 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 24100 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 24200 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 24300 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 24400 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 24500 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 24600 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 24700 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 24800 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 24900 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 25000 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 25100 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 25200 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 25300 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 25400 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 25500 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 25600 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 25700 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 25800 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 25900 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 26000 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 26100 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 26200 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 26300 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 26400 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 26500 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 26600 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 26700 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 26800 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 26900 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 27000 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 27100 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 27200 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 27300 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 27400 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 27500 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 27600 Accuracy = 0.99813 Cost =  101.14655\n",
      "Training step: 27700 Accuracy = 0.99815 Cost =  101.14654\n",
      "Training step: 27800 Accuracy = 0.99815 Cost =  101.14654\n",
      "Training step: 27900 Accuracy = 0.99815 Cost =  101.14654\n",
      "Training step: 28000 Accuracy = 0.99817 Cost =  101.14651\n",
      "Training step: 28100 Accuracy = 0.99817 Cost =  101.14651\n",
      "Training step: 28200 Accuracy = 0.99817 Cost =  101.14651\n",
      "Training step: 28300 Accuracy = 0.99817 Cost =  101.14651\n",
      "Training step: 28400 Accuracy = 0.99817 Cost =  101.14651\n",
      "Training step: 28500 Accuracy = 0.99817 Cost =  101.14651\n",
      "Training step: 28600 Accuracy = 0.99817 Cost =  101.14651\n",
      "Training step: 28700 Accuracy = 0.99817 Cost =  101.14651\n",
      "Training step: 28800 Accuracy = 0.99817 Cost =  101.14651\n",
      "Training step: 28900 Accuracy = 0.99817 Cost =  101.14651\n",
      "Training step: 29000 Accuracy = 0.99817 Cost =  101.14651\n",
      "Training step: 29100 Accuracy = 0.99817 Cost =  101.14651\n",
      "Training step: 29200 Accuracy = 0.99817 Cost =  101.14651\n",
      "Training step: 29300 Accuracy = 0.99819 Cost =  101.14650\n",
      "Training step: 29400 Accuracy = 0.99819 Cost =  101.14650\n",
      "Training step: 29500 Accuracy = 0.99819 Cost =  101.14650\n",
      "Training step: 29600 Accuracy = 0.99819 Cost =  101.14650\n",
      "Training step: 29700 Accuracy = 0.99819 Cost =  101.14650\n",
      "Training step: 29800 Accuracy = 0.99819 Cost =  101.14650\n",
      "Training step: 29900 Accuracy = 0.99819 Cost =  101.14650\n",
      "\n",
      "Optimization Finished!\n",
      "Training Accuracy= 0.998189\n",
      "\n",
      "Testing Accuracy= 0.996994\n"
     ]
    }
   ],
   "source": [
    "accuracy_summary = []\n",
    "cost_summary = [] \n",
    "\n",
    "for i in range(training_epochs):  \n",
    "    sess.run([optimizer], feed_dict={x: inputX, y_: inputY})\n",
    "    \n",
    "    \n",
    "    if (i) % display_step == 0:\n",
    "        train_accuracy, newCost = sess.run([accuracy, cost], feed_dict={x: inputX, y_: inputY})\n",
    "        print (\"Training step:\", i,\n",
    "               \"Accuracy =\", \"{:.5f}\".format(train_accuracy), \n",
    "               \"Cost = \", \"{:.5f}\".format(newCost))\n",
    "        accuracy_summary.append(train_accuracy)\n",
    "        cost_summary.append(newCost)\n",
    "        \n",
    "print()\n",
    "print (\"Optimization Finished!\")\n",
    "training_accuracy = sess.run(accuracy, feed_dict={x: inputX, y_: inputY})\n",
    "print (\"Training Accuracy=\", training_accuracy)\n",
    "print()\n",
    "testing_accuracy = sess.run(accuracy, feed_dict={x: inputX_test, y_: inputY_test})\n",
    "print (\"Testing Accuracy=\", testing_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmoAAAEWCAYAAADW2rtYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xu4XHdd7/H3d/Yl9+5cTdOmaVoIKJc2hVhEKILcCnJs\nRU9pVSgIp+IDHo7A0epRUNQDoiAiSKnQQxVorSKlKsilltujlSalTS/2fk2a5tKkae57z8z3/DFr\nZ0929s6+zMye2Tvv1/PMM2v91pq1vvPLNP1k/dYlMhNJkiR1nlK7C5AkSdLIDGqSJEkdyqAmSZLU\noQxqkiRJHcqgJkmS1KEMapIkSR3KoCZJktShDGqSpq2I+HZE7IqIWe2uRZJawaAmaVqKiNXAOUAC\nPzuF++2eqn1JkkFN0nT1JuBG4HPAxYONETEnIj4SEQ9HxO6I+H5EzCmWvTgi/j0inoyIRyPizUX7\ntyPibXXbeHNEfL9uPiPiHRFxL3Bv0fYXxTaeiogNEXFO3fpdEfE7EXF/ROwplp8SEZ+MiI/Uf4mI\nuC4ifqMVHSRp+jOoSZqu3gR8oXi9OiKWF+1/Bjwf+ElgMfCbQDUiTgW+BvwlsAxYC9wygf2dD7wA\neFYxf1OxjcXAF4G/j4jZxbJ3AxcBrwVOAH4F2A9cCVwUESWAiFgKvKL4vCQdxaAmadqJiBcDpwLX\nZOYG4H7gF4sA9CvAuzJzc2ZWMvPfM/MQ8IvAtzLzqswcyMwnMnMiQe2DmbkzMw8AZObni22UM/Mj\nwCzgmcW6bwN+NzPvzppbi3V/AOwGXl6sdyHw7czc2mCXSJqhDGqSpqOLgW9k5o5i/otF21JgNrXg\nNtwpo7SP16P1MxHx3oj4r2J49Umgr9j/WPu6EvjlYvqXgb9toCZJM5wnxUqaVorzzS4AuiLi8aJ5\nFrAQWAEcBJ4G3Drso48CZ4+y2X3A3Lr5E0dYJ+tqOIfakOrLgTsysxoRu4Co29fTgNtH2M7ngdsj\n4kzgx4BrR6lJkjyiJmnaOR+oUDtXbG3x+jHge9TOW7sC+GhEnFSc1P/C4vYdXwBeEREXRER3RCyJ\niLXFNm8BXh8RcyPi6cBbx6hhAVAGtgPdEfE+aueiDfoM8IcRsSZqzoiIJQCZuYna+W1/C3xpcChV\nkkZiUJM03VwM/L/MfCQzHx98AZ8Afgm4FLiNWhjaCfwJUMrMR6id3P+eov0W4Mxim38O9ANbqQ1N\nfmGMGr4O/CtwD/AwtaN49UOjHwWuAb4BPAV8FphTt/xK4Lk47ClpDJGZY68lSWqaiHgJtSHQU9O/\nhCUdg0fUJGkKRUQP8C7gM4Y0SWMxqEnSFImIHwOepHbRw8faXI6kacChT0mSpA7lETVJkqQONSPu\no7Z06dJcvXp1u8uQJEka04YNG3Zk5rLxrDsjgtrq1atZv359u8uQJEkaU0Q8PN51p3ToMyKuiIht\nETHS3bopbgz58Yi4LyI2RsTzprI+SZKkTjLV56h9Djj3GMtfA6wpXpcAn5qCmiRJkjrSlA59ZuZ3\nI2L1MVY5D/ib4t5CN0bEwohYkZlbpqTA40C1mhwsVzg4UOXgQIUDAxUODtTmD5Ur9JertVeletT0\nQCWpVKtUqlCpVilXk0omlUpSribVrL1XKkV7dehVLj5XzSQzSSCT4n3oyuNaW9be66aheNBi/fK6\nz9ZvjyO2n0PbHf6Zw9s9cnt5eGdHtg2vbajm2kxEUCpBKYJSBBGD0xTztenuUtBVCrpLJUol6C6V\n6Cra6vdVratzpD7JEfpieL8evf5QvdUc+XsPzlen8ILwWd0llszvpaerVPsdVYrfzrDfUaU69OdN\n3Xc5XGpyzOX1f+ZHzLfx4vd2XXnfrq/cjq+bbfu2nc2bPozsJ5+2hI9deFa7yzis085RO5kjH8Oy\nqWg7KqhFxCXUjrqxatWqKSmu1cqVKrsPDPDkgQH2HxoKUYPvhwaqRwSrw+3lCgf6i/A1OF2ucrC/\nUoSyobb+crVp9Q4Gjq7DwaN+elgIiVp7qQRBLcQEQARRewMopofagtqKMbisBEGptiyGtjXoyM/W\n5oe2W5saWlZsf3B6+P6Gfba+NoZ9FgbDYC2wVnMoCFXr2+rCx2AYOVCp1AJutTrUNyN8j/qaIyj6\ncuS+GAyKwz87+P1LdeuP9L1Lw/qg1Q70V3hi3yEOlcuHfyu93aUjflulon3ot1L3oxl6O+LPDahb\nf+TlDNveFH3lI7Rhl7X9tmnH0YZv3K7v2unsl6M9Y/mCdpdwhE4LauOWmZcDlwOsW7eu4/9dcKC/\nwr3b9nD343t4cMc+duw9xBN7+9mxr59d+/rZtb+fPQfLE9pmT1cwu6ereJWYc3i6i745PSxfMIs5\nvV3M7q4tn314uos5PaXae28Xs4rls7q76O0uMau7RE9Xid7u4tU19H74f5ol/+uWJKnVOi2obQZO\nqZtfWbRNK5nJzY88yQ8f2cVtm3dz26bdPPjEvsOHmXu6giXzZrFkfi+L5/WyeslcFs3tpW9OD4vm\n9rBwbi/zZnUfFb5mD4arYr7LsCRJ0ozWaUHtOuCdEXE18AJg93Q7P+1bd27lo9+8hzu3PAXAir7Z\nPPfkPn527Un86IkLeOaJJ7Bq8VxDliRJGtOUBrWIuAp4KbA0IjYB7wd6ADLzMuCrwGuB+4D9wFum\nsr5Gffo79/PBr93F6cvm8eGfP4OX/ugyfmTB7HaXJUmSpqmpvurzojGWJ/COKSqnqT5/48N88Gt3\n8TNnrOCjF5zJrO6udpckSZKmuU4b+py2/mXjFn70xAV8/MKzHNaUJElN4UPZm6BcqXLrpif5idOX\nGNIkSVLTGNSa4K7H97C/v8JZqxa2uxRJkjSDGNSa4OZHdgHw/FMXtbkSSZI0kxjUmuDmh3ex/IRZ\nnLxwTrtLkSRJM4hBrQk2PLKL561aNGWP25EkSccHg1qDtj11kEd3HnDYU5IkNZ1BrUF3b90DwLNP\n6mtzJZIkaaYxqDXo0EAVgPmzvCWdJElqLoNagwYqtaDW0+35aZIkqbkMag3qHwxqXXalJElqLtNF\ng8qVBKDXoCZJkprMdNGgwaHP7i6HPiVJUnMZ1Bo04NCnJElqEdNFg/qLoU+DmiRJajbTRYMGj6h5\njpokSWo200WDBsqDQ5+eoyZJkprLoNaggWpt6LOrZFCTJEnNZVBr0EClSm9XyQeyS5KkpjOoNWig\nXHXYU5IktYRBrUEDlSo93XajJElqPhNGg/or6a05JElSS5gwGlSuVOnxQgJJktQCBrUGOfQpSZJa\nxYTRoAGHPiVJUouYMBrUX6ka1CRJUkuYMBpUu4+a56hJkqTmM6g1qFxJuj2iJkmSWsCE0aDa0KdH\n1CRJUvMZ1Bo04DlqkiSpRUwYDRp81qckSVKzmTAaNFD29hySJKk1TBgNGqhU6fYcNUmS1AIGtQYN\nVB36lCRJrWHCaJBDn5IkqVVMGA2qPevToU9JktR8Ux7UIuLciLg7Iu6LiEtHWP7SiNgdEbcUr/dN\ndY0T4SOkJElSq3RP5c4iogv4JPBKYBNwU0Rcl5l3Dlv1e5n5uqmsbbK8PYckSWqVqU4YZwP3ZeYD\nmdkPXA2cN8U1NFXtEVIOfUqSpOab6qB2MvBo3fymom24n4yIjRHxtYh49kgbiohLImJ9RKzfvn17\nK2odU7WalKteTCBJklqjExPGzcCqzDwD+Evg2pFWyszLM3NdZq5btmzZlBY4aKBaBTCoSZKklpjq\nhLEZOKVufmXRdlhmPpWZe4vprwI9EbF06kocv4FKAniOmiRJaompThg3AWsi4rSI6AUuBK6rXyEi\nToyIKKbPLmp8YorrHJeB8uARNc9RkyRJzTelV31mZjki3gl8HegCrsjMOyLi7cXyy4BfAH4tIsrA\nAeDCzMyprHO8Boc+uz2iJkmSWmBKgxocHs786rC2y+qmPwF8YqrrmgyHPiVJUiuZMBpweOjTJxNI\nkqQWMKg1YKDiVZ+SJKl1TBgN6DeoSZKkFjJhNKBcnKPmVZ+SJKkVDGoNcOhTkiS1kgmjAQ59SpKk\nVjJhNGDg8NCn3ShJkprPhNGAwdtzeB81SZLUCiaMBgyeo9btxQSSJKkFDGoNGKg69ClJklrHhNEA\nhz4lSVIrmTAacPj2HD5CSpIktYBBrQHeR02SJLWSCaMB/YO35yjZjZIkqflMGA0oO/QpSZJayKDW\nAIc+JUlSK5kwGjA49Nld8oiaJElqPoNaAwYqVXq7SkQY1CRJUvMZ1BowUK76VAJJktQyBrUGlKvp\n+WmSJKllTBkN6K9UDWqSJKllTBkNGChX6XXoU5IktYhBrQEDlSo93XahJElqDVNGAwYq6a05JElS\nyxjUGjDgOWqSJKmFTBkNGKhU6XXoU5IktYgpowEDFW/PIUmSWseU0YDa7Tk8R02SJLWGQa0BnqMm\nSZJayZTRAIOaJElqJVNGA8qVdOhTkiS1jEGtAT5CSpIktZIpowEDlSq9BjVJktQipowGDJSTboc+\nJUlSixjUGuDFBJIkqZVMGQ0wqEmSpFaa8pQREedGxN0RcV9EXDrC8oiIjxfLN0bE86a6xvHITPp9\nhJQkSWqh7qncWUR0AZ8EXglsAm6KiOsy88661V4DrCleLwA+Vby3TWayv79yeL5cSf7gn+7g4ECV\n1UvmtbEySZI0k01pUAPOBu7LzAcAIuJq4DygPqidB/xNZiZwY0QsjIgVmbllims97KmDZc78g28c\n1f6eVz6Di84+pQ0VSZKk48FUB7WTgUfr5jdx9NGykdY5GTgiqEXEJcAlAKtWrWp6ofVmdZf4ndf+\n6BFtzz6pjxc9fWlL9ytJko5vUx3UmiYzLwcuB1i3bl22cl+ze7q45CVPa+UuJEmSjjLVZ8JvBurH\nClcWbRNdR5Ikacab6qB2E7AmIk6LiF7gQuC6YetcB7ypuPrzJ4Dd7Tw/TZIkqV2mdOgzM8sR8U7g\n60AXcEVm3hERby+WXwZ8FXgtcB+wH3jLWNvdsGHDjoh4uHWVH7YU2DEF+zme2KfNZX82n33afPZp\n89mnzdfKPj11vCtG7eJKjUdErM/Mde2uYyaxT5vL/mw++7T57NPms0+br1P61Lu1SpIkdSiDmiRJ\nUocyqE3M5e0uYAayT5vL/mw++7T57NPms0+bryP61HPUJEmSOpRH1CQddyLiFyNifUTsjYgtEfG1\niHhxA9t7KCJe0cwaJQkMapKOMxHxbuBjwP8FlgOrgE8CP9vOuiRpJA59SjpuREQftSedvCUz/36E\n5bOAPwEuKJquAX4rMw9FxFLgc8CLgSpwB/BTwJXALwGHgArwgcz8cIu/iqTjhEfUJB1PXgjMBr48\nyvL/A/wEsBY4Ezgb+N1i2XuATcAyakfifgfIzHwj8Ajw3zJzviFNUjMZ1CQdT5YAOzKzPMryX6J2\nRGxbZm4H/gB4Y7FsAFgBnJqZA5n5vXRIQlKLGdQkHU+eAJZGxGiPzzsJqH8c3cNFG8CfUnu03Tci\n4oGIuLR1ZUpSjUFN0vHkP6idS3b+KMsf48hn8K0q2sjMPZn5nsw8ndqFB++OiJcX63lkTVJLTOlD\n2SWpnTJzd0S8D/hkRJSBb1Ab0nwF8DLgKuB3I+ImauHrfcDnASLidcBdwP3AbmoXDlSLTW8FTp/C\nryLpOOERNUnHlcz8CPBuahcJbAceBd4JXAv8EbAe2AjcBtxctAGsAb4F7KV2ZO6vMvOGYtkHqQW8\nJyPivVP0VSQdB7w9hyRJUofyiJokSVKHMqhJkiR1KIOaJElShzKoSZIkdagZcXuOpUuX5urVq9td\nhiRJ0pg2bNiwIzOXjWfdGRHUVq9ezfr169tdhiRJ0pgi4uGx16px6FOSJKlDzYgjaq1WrlR5ZOd+\ndu7rp7urxNpTFra7JEmSdBwwqI3DUwfL/PRHvnN4/nu/+TJOWTy3jRVJkqTjgUOf47BwTg8fveBM\n3ve6ZwFwz9Y9ba5IkiQdDwxq41AqBa9/3kpe/7yTAXhg+742VyRJko4HBrUJWDi3l0Vze3hgh0FN\nkiS1nkFtgk5fNp8Htu9tdxmSJOk4YFCboNOXzvOImiRJmhIGtQk6bdk8tu85xJ6DA+0uRZIkzXAG\ntQk6fel8AB70qJokSWoxg9oEnb5sHmBQkyRJrWdQm6BTl8wlAu73Fh2SJKnFDGoTNKu7i5WL5nhE\nTZIktZxBbRJOXzqfB3d4iw5JktRaBrVJWH7CLLbvOdTuMiRJ0gxnUJuERfN62bVvgMxsdymSJGkG\nM6hNwuK5vfRXquzrr7S7FEmSNIONGdQi4oqI2BYRt9e1LY6Ib0bEvcX7oqJ9SUTcEBF7I+ITx9jm\n30XELcXroYi4pWhfHREH6pZd1owv2WyL5/UCsGtff5srkSRJM9l4jqh9Djh3WNulwPWZuQa4vpgH\nOAj8HvDeY20wM9+QmWszcy3wJeAf6xbfP7gsM98+jvqm3GBQ22lQkyRJLTRmUMvM7wI7hzWfB1xZ\nTF8JnF+suy8zv08tsI0pIgK4ALhqvAV3gkWDQW2/QU2SJLXOZM9RW56ZW4rpx4Hlk9zOOcDWzLy3\nru20YtjzOxFxzmgfjIhLImJ9RKzfvn37JHc/OYvnOvQpSZJar+GLCbJ26eNkL3+8iCOPpm0BVhVD\nou8GvhgRJ4yy38szc11mrlu2bNkkdz85ixz6lCRJU2CyQW1rRKwAKN63TXQDEdENvB74u8G2zDyU\nmU8U0xuA+4FnTLLGljlhdjfdpTCoSZKklppsULsOuLiYvhj4yiS28QrgrszcNNgQEcsioquYPh1Y\nAzwwyRpbJiJq91LzHDVJktRC47k9x1XAfwDPjIhNEfFW4EPAKyPiXmqB60N16z8EfBR4c7H+s4r2\nz0TEurpNX8jRFxG8BNhY3K7jH4C3Z+bwCxk6wuK5vR5RkyRJLdU91gqZedEoi14+yvqrR2l/27D5\nN4+wzpeo3a6j4y2a18OufQPtLkOSJM1gPplgkhbP6/X2HJIkqaUMapO0aG6vt+eQJEktZVCbpMXF\nxQTVqg9mlyRJrWFQm6TF83qpJuw+4HlqkiSpNQxqk7TYx0hJkqQWM6hN0iIfIyVJklrMoDZJi32M\nlCRJajGD2iQNPu/TpxNIkqRWMahN0uK5g0fUvJhAkiS1hkFtkub0djG3t4vtew61uxRJkjRDGdQa\ncGLfbLY+dbDdZUiSpBnKoNaAFX2zeWz3gXaXIUmSZiiDWgNW9M3h8d0eUZMkSa1hUGvAir7ZbNtz\niHKl2u5SJEnSDGRQa8CJfbOpVJMde71FhyRJaj6DWgNO6psD4HlqkiSpJQxqDTixbzaA56lJkqSW\nMKg1YEUR1LYY1CRJUgsY1BrQN6eH2T0lHnfoU5IktYBBrQERwUl9c3jMI2qSJKkFDGoNOrFvtueo\nSZKklhgzqEXEFRGxLSJur2tbHBHfjIh7i/dFRfuSiLghIvZGxCeOsc3fj4jNEXFL8Xpt3bLfjoj7\nIuLuiHh1o1+w1QxqkiSpVcZzRO1zwLnD2i4Frs/MNcD1xTzAQeD3gPeOY7t/nplri9dXASLiWcCF\nwLOLff5VRHSNY1tts6J43melmu0uRZIkzTBjBrXM/C6wc1jzecCVxfSVwPnFuvsy8/vUAttknAdc\nnZmHMvNB4D7g7Elua0qs6JtDuZrs2Huo3aVIkqQZZrLnqC3PzC3F9OPA8kls49cjYmMxtLqoaDsZ\neLRunU1F21Ei4pKIWB8R67dv3z6J3TfHSQtrt+jYtMsrPyVJUnM1fDFBZiYw0XG/TwGnA2uBLcBH\nJrHfyzNzXWauW7Zs2UQ/3jTPPPEEAO54bHfbapAkSTPTZIPa1ohYAVC8b5vIhzNza2ZWMrMK/DVD\nw5ubgVPqVl1ZtHWsk/pms3T+LG591KAmSZKaa7JB7Trg4mL6YuArE/nwYMgr/BwweEXpdcCFETEr\nIk4D1gA/mGSNUyIiOHNlHxs3PdnuUiRJ0gzTPdYKEXEV8FJgaURsAt4PfAi4JiLeCjwMXFC3/kPA\nCUBvRJwPvCoz74yIzwCXZeZ64MMRsZbakOlDwK8CZOYdEXENcCdQBt6RmZUmfdeWOWPlQv7t7m3s\nPVRm/qwxu1SSJGlcxkwVmXnRKItePsr6q0dpf1vd9BuPsb8/Bv54rLo6yRmn9JEJt23azQuftqTd\n5UiSpBnCJxM0wZkrFwI4/ClJkprKoNYEi+f1snLRHDZu8oICSZLUPAa1Jjlz5UJ++MguancrkSRJ\napxBrUnOWbOUx3Yf5I7Hnmp3KZIkaYYwqDXJq599It2l4J83bhl7ZUmSpHEwqDXJonm9vOjpS/nn\njY85/ClJkprCoNZEP3PGCjbtOuBFBZIkqSkMak306medSE9X8NXbHP6UJEmNM6g1Ud/cHs46ZRE3\nPriz3aVIkqQZwKDWZGedupA7H9vNwYGOf/KVJEnqcAa1JnveqkUMVJI7HvM8NUmS1BiDWpOdtar2\nOKkfPuLjpCRJUmMMak32Iwtms3LRHG5+ZFe7S5EkSdOcQa0Fzlq1iJsf9oiaJElqjEGtBZ63aiGP\nP3WQLbsPtLsUSZI0jRnUWuD5py4C4Nt3b29zJZIkaTozqLXAc0/u48dWnMBnv/8g1aqPk5IkSZNj\nUGuBiOCSl5zGfdv28u17trW7HEmSNE0Z1FrkdWecxIq+2Xz6Ow/4kHZJkjQpBrUW6ekqcclLTuc/\nH9zJP968ud3lSJKkacig1kJveuFqzj5tMe/7yu08tGNfu8uRJEnTzJhBLSKuiIhtEXF7XdviiPhm\nRNxbvC8q2pdExA0RsTciPnGMbf5pRNwVERsj4ssRsbBoXx0RByLiluJ1WTO+ZLt0lYKPvWEtXaXg\nbX+znu17DrW7JEmSNI2M54ja54Bzh7VdClyfmWuA64t5gIPA7wHvHWOb3wSek5lnAPcAv1237P7M\nXFu83j6O+jraSQvn8Ok3rmPzrgNc9Nc3GtYkSdK4jRnUMvO7wM5hzecBVxbTVwLnF+vuy8zvUwts\nx9rmNzKzXMzeCKycSNHTzQuftoTPveXH2bRrP79+1c2UK9V2lyRJkqaByZ6jtjwztxTTjwPLG6jh\nV4Cv1c2fVgx7ficizhntQxFxSUSsj4j127d3/o1lX3D6Ev7o/Ody4wM7+fNv3dPuciRJ0jTQ3egG\nMjMjYlL3n4iI/wOUgS8UTVuAVZn5REQ8H7g2Ip6dmU+NsN/LgcsB1q1bNy3uf/ELz1/JTQ/u5JM3\n3E8mvOdVz6SrFO0uS5IkdajJBrWtEbEiM7dExApgwnd1jYg3A68DXp7FjcYy8xBwqJjeEBH3A88A\n1k+yzo7zh+c/h1Ip+Ktv388PH3mSD5z3bNYsX9DusiRJUgea7NDndcDFxfTFwFcm8uGIOBf4TeBn\nM3N/XfuyiOgqpk8H1gAPTLLGjtTbXeKDr38uf/Lzz+WOx3Zz7l98j//997dy79Y97S5NkiR1mBjr\nrvkRcRXwUmApsBV4P3AtcA2wCngYuCAzdxbrPwScAPQCTwKvysw7I+IzwGWZuT4i7gNmAU8Uu7kx\nM98eET8PfAAYAKrA+zPzn8b6EuvWrcv166ffQbed+/r5+PX3cvVNj3BwoMpZqxZy/tqTed0ZK1gy\nf1a7y5MkSS0QERsyc9241p0JjzearkFt0M59/Vyz/lGu/eFm7np8D12l4Jw1Szlv7Um86OlL+ZEF\ns9tdoiRJahKD2jR29+N7uPaWzVx3y2NsfvIAAKuXzGXd6sWcsbKPNT+ygGcsn+8RN0mSpimD2gxQ\nrSYbN+/mpgd38oOHdrL+oZ3s2j9wePmSeb2sWT6f05fN59TFc1m1eC6rltTeF8zuaWPlkiTpWAxq\nM1Bm8vhTB7ln617u3bqHe7fu5Z5te3hox74jAhzA4nm9nLJ47uEAt2zBLJbOn8XS+b0sXTCLpfNm\nccKcbiK8NYgkSVNtIkGt4fuoaWpEBCv65rCibw4/9YxlRyx76uAAjzyxn0d21r2e2M8tjz7Jv9y2\nhUr16DDe21Vi0bweFszuYcHs7qH3Wd1HzM/t7WJ2Txdzeor33qHp2T0l5hRtvV0lukph+JMkqYkM\najPACbN7eM7JfTzn5L6jllWqya79/ezYe4gde4r3vYfYsbefnfsOsedgmT0Hy+w+MMCmXfuL+QEO\nDkz8MVcR0FMq0d0V9HSV6CneD8+XSvR0B92l+mUleoqA11WCUkTtVQpKUZuPgK7D7XXrBMV6R093\nRW2b9cuAofliu/XLo5gORlinNDh/5DpRt71SiSP3GUFQt06p/jND6xyuqzRUz1HbiSO/9xH7jSDq\nP0tt/frPRPHnY5CWpOnFoDbDdZWiGPacBSeO/3MDlSp7DpY5MFDhQH+FgwO11+H5cpWD/cX8QIX+\ncpVypUp/JSlXqgxUqgxUk4FylXI1a/OVKuVK0j/4Xq6yr79CuVKlmrXh3WomlWqSSW06k2p1cBlU\nMoemq7X1c/h0Dk3raPXhL4oQVx8AI6i1leKIsFkLfozQNsJnBwNi/T5Kx/hsES57u0vM6i4xq7ur\n9t7TRXddwIX64Fq/z8F1hvbBBELpROLrRLJuTGDLE9vuBNadyHZbFORndJ9NaMvT2/Hy77yVi+Zy\n7nMm8D/MFjOoaUQ9XSUWz+ttdxkNyRwKfNXiHY6cz+rgfNaFRUiKdar12xhhneLzo60zuKx+vjpC\nXeNbp355sX71yG1UBrfF0dscnE/qt1HXVh36Xkd9doT+HHl7Q/s/3Dcjffaotlr7vkNldu6rcnCg\nwqFylYMDVSrV6uH6ao8wGdru0Pca+jMbrFuSJuOlz1xmUJOmwuGjNsfRv3jVWhO5+GoiR3Qnkisn\nVMOEtjuBdSew5VYd2e6Eeo+nfw/MhAsPx6vTnsFtUJOkcZrI0GDrhok6638iklprss/6lCRJUosZ\n1CRJkjrUjLjhbURsp/Zw+FZbCuyYgv0cT+zT5rI/m88+bT77tPns0+ZrZZ+empnLxl5thgS1qRIR\n68d7J2GNj33aXPZn89mnzWefNp992nyd0qcOfUqSJHUog5okSVKHMqhNzOXtLmAGsk+by/5sPvu0\n+ezT5rN/3VUCAAAHTElEQVRPm68j+tRz1CRJkjqUR9QkSZI6lEFNkiSpQxnUxiEizo2IuyPivoi4\ntN31TFcR8VBE3BYRt0TE+qJtcUR8MyLuLd4XtbvOThYRV0TEtoi4va5t1D6MiN8ufrd3R8Sr21N1\nZxulT38/IjYXv9VbIuK1dcvs02OIiFMi4oaIuDMi7oiIdxXt/k4n6Rh96u90kiJidkT8ICJuLfr0\nD4r2jvudeo7aGCKiC7gHeCWwCbgJuCgz72xrYdNQRDwErMvMHXVtHwZ2ZuaHihC8KDN/q101drqI\neAmwF/ibzHxO0TZiH0bEs4CrgLOBk4BvAc/IzEqbyu9Io/Tp7wN7M/PPhq1rn44hIlYAKzLz5ohY\nAGwAzgfejL/TSTlGn16Av9NJidqDe+dl5t6I6AG+D7wLeD0d9jv1iNrYzgbuy8wHMrMfuBo4r801\nzSTnAVcW01dS+8tHo8jM7wI7hzWP1ofnAVdn5qHMfBC4j9rvWXVG6dPR2KdjyMwtmXlzMb0H+C/g\nZPydTtox+nQ09ukYsmZvMdtTvJIO/J0a1MZ2MvBo3fwmjv0fiEaXwLciYkNEXFK0Lc/MLcX048Dy\n9pQ2rY3Wh/52G/PrEbGxGBodHP6wTycgIlYDZwH/ib/TphjWp+DvdNIioisibgG2Ad/MzI78nRrU\nNJVenJlrgdcA7yiGnA7L2ji8Y/ENsA+b5lPA6cBaYAvwkfaWM/1ExHzgS8D/ysyn6pf5O52cEfrU\n32kDMrNS/D9pJXB2RDxn2PKO+J0a1Ma2GTilbn5l0aYJyszNxfs24MvUDhtvLc6/GDwPY1v7Kpy2\nRutDf7uTlJlbi7/Eq8BfMzTEYZ+OQ3HOz5eAL2TmPxbN/k4bMFKf+jttjsx8ErgBOJcO/J0a1MZ2\nE7AmIk6LiF7gQuC6Ntc07UTEvOIkWCJiHvAq4HZqfXlxsdrFwFfaU+G0NlofXgdcGBGzIuI0YA3w\ngzbUN+0M/kVd+Dlqv1WwT8dUnKT9WeC/MvOjdYv8nU7SaH3q73TyImJZRCwspudQu2DwLjrwd9o9\nFTuZzjKzHBHvBL4OdAFXZOYdbS5rOloOfLn29w3dwBcz818j4ibgmoh4K/AwtauYNIqIuAp4KbA0\nIjYB7wc+xAh9mJl3RMQ1wJ1AGXiHV30dbZQ+fWlErKU27PEQ8Ktgn47Ti4A3ArcV5/8A/A7+Thsx\nWp9e5O900lYAVxZ3digB12TmP0fEf9Bhv1NvzyFJktShHPqUJEnqUAY1SZKkDmVQkyRJ6lAGNUmS\npA5lUJMkSepQBjVJHSciKhFxS93r0iZue3VE3D72mqN+/qyI+OwY67wkIm6OiHJE/MKwZRdHxL3F\n6+K69tMi4j8j4r6I+Lvivo1ExOsi4gOTrVfS9ObtOSR1nIjYm5nzW7Tt1cA/Z+Zzxlh1tM//PfBH\nmXnrGPs4AXgvcF1m/kPRvhhYD6yjdu+rDcDzM3NXcY+mf8zMqyPiMuDWzPxUcbPTm4EXZeb+ydQs\nafryiJqkaSMiHoqID0fEbRHxg4h4etG+OiL+rXg49fURsapoXx4RX46IW4vXTxab6oqIv46IOyLi\nG8WdyYmI/xkRdxbbuXqE/S8AzhgMaRHxFxHxvmL61RHx3YgoZeZDmbkRqA7bxKupPfx5Z2buAr4J\nnFuEsZ8G/qFY70rgfDj8vMFvA69rTi9Kmk4MapI60ZxhQ59vqFu2OzOfC3wC+FjR9pfAlZl5BvAF\n4ONF+8eB72TmmcDzgMGniqwBPpmZzwaeBH6+aL8UOKvYzttHqGsdQ4/pAfht4A0R8bJiX28pnrs4\nmpOBR+vmNxVtS4AnM7M8rH3QeuCcY2xX0gzlI6QkdaIDmbl2lGVX1b3/eTH9QuD1xfTfAh8upn8a\neBNA8biX3RGxCHgwMwcfxbMBWF1MbwS+EBHXAteOsO8VwPbBmczcHxH/A/gu8BuZef+4v+HEbANO\natG2JXUwj6hJmm5ylOmJOFQ3XWHoH60/A3yS2tG3myJi+D9mDwCzh7U9F3iC8QWpzcApdfMri7Yn\ngIV1+xtsHzS72Lek44xBTdJ084a69/8opv8duLCY/iXge8X09cCvAUREV0T0jbbRiCgBp2TmDcBv\nAX3A8Asa/gt4et1nTgXeA5wFvCYiXjBG7V8HXhURi4oje68Cvl6ch3YDMHiF6MXAV+o+9wyOHHKV\ndJwwqEnqRMPPUftQ3bJFEbEReBfwG0XbrwNvKdrfWCyjeH9ZRNxGbYjzWcfYZxfw+WLdHwIfz8wn\n61fIzLuAvohYUFwA8FngvZn5GPBW4DMRMTsifjwiNgH/Hfh0RNxRfH4n8IfATcXrA0Ub1MLhuyPi\nPmrnrNXfAuRlwL+M3W2SZhpvzyFp2oiIh4B1mbmjjTX8BrAnMz8zRftbDnwxM18+FfuT1Fk8oiZJ\nE/MpjjzHrdVWURtelXQc8oiaJElSh/KImiRJUocyqEmSJHUog5okSVKHMqhJkiR1KIOaJElSh/r/\nt1xHPqmbiDMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff8fafff6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(10,4))\n",
    "\n",
    "ax1.plot(accuracy_summary)\n",
    "ax1.set_title('Accuracy')\n",
    "\n",
    "ax2.plot(cost_summary)\n",
    "ax2.set_title('Cost')\n",
    "\n",
    "plt.xlabel('Epochs (x100)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.996994\n",
      "[[   204     42]\n",
      " [   663 233652]]\n",
      "RECALL SCORE 0.997170475642\n",
      "PRECISION SCORE 0.999820277799\n"
     ]
    }
   ],
   "source": [
    "predicted = tf.argmax(y, 1)\n",
    "testing_accuracy, testing_predictions = sess.run([accuracy,predicted], feed_dict={x: inputX_test, y_: inputY_test })\n",
    "print(testing_accuracy)\n",
    "print(confusion_matrix(y_test.Normal, testing_predictions))\n",
    "print(\"RECALL SCORE\",recall_score(y_test.Normal,testing_predictions))\n",
    "print(\"PRECISION SCORE\", precision_score(y_test.Normal,testing_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fraud</th>\n",
       "      <th>Normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>151991</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92317</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238936</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184752</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117160</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246800</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275474</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138690</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16327</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197300</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252043</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162328</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76536</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6650</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158155</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272029</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17848</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239725</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78154</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67356</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119923</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81556</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218231</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284767</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176696</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208740</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9346</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28700</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71436</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109104</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228479</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233341</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254036</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107629</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66875</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103457</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6638</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156288</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202778</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104667</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215591</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30016</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196606</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267433</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274910</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171921</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112034</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101681</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135097</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246342</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168869</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125563</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184836</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154340</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278821</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59740</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276108</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99058</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127357</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>234561 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Fraud  Normal\n",
       "151991    0.0     1.0\n",
       "92317     0.0     1.0\n",
       "238936    0.0     1.0\n",
       "184752    0.0     1.0\n",
       "117160    0.0     1.0\n",
       "246800    0.0     1.0\n",
       "275474    0.0     1.0\n",
       "138690    0.0     1.0\n",
       "16327     0.0     1.0\n",
       "197300    0.0     1.0\n",
       "252043    0.0     1.0\n",
       "162328    0.0     1.0\n",
       "76536     0.0     1.0\n",
       "6650      0.0     1.0\n",
       "158155    0.0     1.0\n",
       "272029    0.0     1.0\n",
       "17848     0.0     1.0\n",
       "239725    0.0     1.0\n",
       "78154     0.0     1.0\n",
       "67356     0.0     1.0\n",
       "119923    0.0     1.0\n",
       "81556     0.0     1.0\n",
       "218231    0.0     1.0\n",
       "284767    0.0     1.0\n",
       "176696    0.0     1.0\n",
       "208740    0.0     1.0\n",
       "9346      0.0     1.0\n",
       "28700     0.0     1.0\n",
       "71436     0.0     1.0\n",
       "109104    0.0     1.0\n",
       "...       ...     ...\n",
       "228479    0.0     1.0\n",
       "233341    0.0     1.0\n",
       "254036    0.0     1.0\n",
       "107629    0.0     1.0\n",
       "66875     0.0     1.0\n",
       "103457    0.0     1.0\n",
       "6638      0.0     1.0\n",
       "156288    0.0     1.0\n",
       "137998    0.0     1.0\n",
       "202778    0.0     1.0\n",
       "104667    0.0     1.0\n",
       "215591    0.0     1.0\n",
       "30016     0.0     1.0\n",
       "196606    0.0     1.0\n",
       "267433    0.0     1.0\n",
       "274910    0.0     1.0\n",
       "171921    0.0     1.0\n",
       "112034    0.0     1.0\n",
       "101681    0.0     1.0\n",
       "135097    0.0     1.0\n",
       "246342    0.0     1.0\n",
       "168869    0.0     1.0\n",
       "125563    0.0     1.0\n",
       "184836    0.0     1.0\n",
       "154340    0.0     1.0\n",
       "278821    0.0     1.0\n",
       "59740     0.0     1.0\n",
       "276108    0.0     1.0\n",
       "99058     0.0     1.0\n",
       "127357    0.0     1.0\n",
       "\n",
       "[234561 rows x 2 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
